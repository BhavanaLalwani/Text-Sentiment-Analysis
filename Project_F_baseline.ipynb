{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_F_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhavanaLalwani/Text-Sentiment-Analysis/blob/master/Project_F_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_OvaN27oz0a",
        "colab_type": "code",
        "outputId": "f74d07b3-e380-4813-b9dd-86bb82ad9ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAimxh5VXbJP",
        "colab_type": "code",
        "outputId": "552048fb-9182-46b4-ee42-9ac103323068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "path = api.load(\"word2vec-google-news-300\", return_path=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================================--] 97.3% 1617.2/1662.8MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWi4qFJyquQj",
        "colab_type": "code",
        "outputId": "4c396fa6-5174-4412-a714-e156f43dc0d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Upgrade TensorFlow and Keras\n",
        "# !pip install --upgrade tensorflow\n",
        "# !pip install --upgrade keras\n",
        "\n",
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Keras imports\n",
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Input\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model, Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, GlobalMaxPooling1D\n",
        "from keras.initializers import Constant\n",
        "from keras import backend as K\n",
        "import keras\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "BASE_DIR = 'drive/My Drive/ECE542-ProjectF/'\n",
        "DATA_DIR = BASE_DIR + 'data/'\n",
        "GLOVE_DIR = 'drive/My Drive/ECE542-ProjectF/data/GloVe'\n",
        "OUT_DIR = 'drive/My Drive/ECE542-ProjectF/output'\n",
        "ISEAR_PATH = DATA_DIR + 'isear/isear_clean.csv'\n",
        "W2V_PATH = DATA_DIR + 'w2v/GoogleNews-vectors-negative300.bin'\n",
        "# TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "MAX_NUM_WORDS = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "ISEAR_SHUFFLE_SEED = 62\n",
        "KAGGLE_SHUFFLE_SEED = 43\n",
        "\n",
        "ISEAR_TEST_SPLIT = 0.1\n",
        "ISEAR_VALIDATION_SPLIT = 0.1\n",
        "KAGGLE_VALIDATION_SPLIT = 0.2\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTlAO5vkNdT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "\n",
        "def split_data(data, labels, val_split, test_split, seed):\n",
        "  # Shuffle our dataset. Using a known seed allows us to get the same result \n",
        "  # each time we run the script.\n",
        "  indices = np.arange(data.shape[0])\n",
        "  random.Random(seed).shuffle(indices)\n",
        "\n",
        "  data = data[indices]\n",
        "  labels = labels[indices]\n",
        "\n",
        "  # Determine number of samples in train/val/test\n",
        "  num_val = math.floor(data.shape[0] * val_split)\n",
        "  num_test = math.floor(data.shape[0] * test_split)\n",
        "\n",
        "  # Split the data into the test, val, and train subsets.\n",
        "  x_test = data[:num_test]\n",
        "  y_test = labels[:num_test]\n",
        "\n",
        "  x_val = data[num_test:num_val + num_test]\n",
        "  y_val = labels[num_test:num_val + num_test]\n",
        "\n",
        "  x_train = data[num_val + num_test:]\n",
        "  y_train = labels[num_val + num_test:]\n",
        "\n",
        "  # Return a dictionary of the data with train/val/test and x/y.\n",
        "  return {\n",
        "      'train': {\n",
        "          'x': x_train, 'y': y_train\n",
        "      },\n",
        "      'val': {\n",
        "          'x': x_val, 'y': y_val\n",
        "      },\n",
        "      'test': {\n",
        "          'x': x_test, 'y': y_test\n",
        "      }\n",
        "  }\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIPuwREP_9Zm",
        "colab_type": "code",
        "outputId": "b4f4ff89-126f-4977-e120-ef72c81b8848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "data=pandas.read_csv(ISEAR_PATH)\n",
        "data=pandas.DataFrame(data)\n",
        "data.columns=['label','text']\n",
        "\n",
        "x_train, x_val, y_train, y_val = model_selection.train_test_split(data['text'], data['label'])\n",
        "\n",
        "dataset_test = split_data(\n",
        "    data['text'],\n",
        "    data['label'],\n",
        "    ISEAR_VALIDATION_SPLIT, \n",
        "    ISEAR_TEST_SPLIT, \n",
        "    ISEAR_SHUFFLE_SEED\n",
        ")\n",
        "\n",
        "\n",
        "x_train = dataset_test['train']['x']\n",
        "y_train = dataset_test['train']['y']\n",
        "x_val = dataset_test['val']['x']\n",
        "y_val = dataset_test['val']['y']\n",
        "x_test = dataset_test['test']['x']\n",
        "y_test = dataset_test['test']['y']\n",
        "\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, feature_vector_test, is_neural_net=False):\n",
        "  # fit the training dataset on the classifier\n",
        "  classifier.fit(feature_vector_train, label)\n",
        "  # predict the labels on validation dataset\n",
        "  predictions = classifier.predict(feature_vector_valid)\n",
        "  test_predictions = classifier.predict(feature_vector_test)\n",
        "  if is_neural_net:\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "  \n",
        "  return predictions, test_predictions\n",
        "\n",
        "\n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1,1), max_features=10000)\n",
        "tfidf_vect_ngram.fit(data['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(x_train)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(x_val)\n",
        "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(x_test)\n",
        "\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_val = encoder.fit_transform(y_val)\n",
        "y_test = encoder.fit_transform(y_test)\n",
        "\n",
        "val_pred, test_pred = train_model(svm.SVC(), xtrain_tfidf_ngram, y_train, xvalid_tfidf_ngram, xtest_tfidf_ngram)\n",
        "\n",
        "print('Validation Metrics:')\n",
        "report = classification_report(y_val, val_pred)\n",
        "print(report)\n",
        "\n",
        "print('Test Metrics:')\n",
        "report = classification_report(y_test, test_pred, digits=3)\n",
        "print(report)\n",
        "print('F1: {0:0.3f}'.format(f1_score(y_test, test_pred, average='macro')))\n",
        "print('Precision: {0:0.3f}'.format(precision_score(y_test, test_pred, average='macro')))\n",
        "print('Recall: {0:0.3f}'.format(recall_score(y_test, test_pred, average='macro')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.60      0.53       108\n",
            "           1       0.57      0.66      0.62       104\n",
            "           2       0.75      0.72      0.74       108\n",
            "           3       0.45      0.48      0.47       102\n",
            "           4       0.77      0.73      0.75       111\n",
            "           5       0.74      0.58      0.65       114\n",
            "           6       0.48      0.40      0.44        99\n",
            "\n",
            "    accuracy                           0.60       746\n",
            "   macro avg       0.61      0.60      0.60       746\n",
            "weighted avg       0.61      0.60      0.60       746\n",
            "\n",
            "Test Metrics:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.401     0.551     0.465       107\n",
            "           1      0.537     0.607     0.570       107\n",
            "           2      0.694     0.648     0.670       105\n",
            "           3      0.550     0.487     0.516       113\n",
            "           4      0.663     0.704     0.683        98\n",
            "           5      0.681     0.549     0.608       113\n",
            "           6      0.506     0.417     0.457       103\n",
            "\n",
            "    accuracy                          0.564       746\n",
            "   macro avg      0.576     0.566     0.567       746\n",
            "weighted avg      0.576     0.564     0.566       746\n",
            "\n",
            "F1: 0.567\n",
            "Precision: 0.576\n",
            "Recall: 0.566\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}